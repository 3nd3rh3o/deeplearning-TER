{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f31b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install (versions compatibles + évite le package `keras` standalone)\n",
    "%pip install -U pip setuptools wheel\n",
    "%pip install -U tabulate scikit-learn pandas numpy matplotlib seaborn gensim \"tensorflow==2.16.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f715342",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/2007.12673 - Genetic Algorithm: Reviews, Implementations, and Applications - Tanweer Alam, Shamimul Qamar, Amit Dixit, Mohamed Benaida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726cd6d",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3065b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des avertissements liés à Scikit-learn\n",
    "import warnings  # Masquer les avertissements (ex. : FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import gc  # garbage collector (32Gb suffit pas)\n",
    "\n",
    "# Librairies générales\n",
    "import pandas as pd  # Librairie pour la manipulation de données\n",
    "import numpy as np  # Librairie pour le calcul numérique\n",
    "import sys  # Fonctions et variables liées à l'interpréteur Python\n",
    "import copy  # Création de copies d'objets\n",
    "from numpy import mean, std  # Fonctions de calcul de moyenne et d'écart type\n",
    "import zipfile  # Traitement de fichiers zip\n",
    "import os  # Manipulation de fichiers et chemins\n",
    "\n",
    "# Librairie affichage\n",
    "import matplotlib.pyplot as plt  # Outils de visualisation 2D\n",
    "from matplotlib import pyplot  # Interface de la bibliothèque Matplotlib\n",
    "import seaborn as sns  # Bibliothèque de visualisation de données basée sur Matplotlib\n",
    "\n",
    "# Scikit-learn pour l'évaluation des modèles\n",
    "from sklearn.metrics import confusion_matrix  # Matrice de confusion\n",
    "from sklearn.model_selection import KFold  # Outils de validation croisée\n",
    "from sklearn.metrics import accuracy_score  # Calcul de l'accuracy\n",
    "from sklearn.model_selection import train_test_split  # Découpage train/test\n",
    "\n",
    "# TensorFlow et Keras\n",
    "import tensorflow as tf  # Librairie de deep learning\n",
    "import keras  # API haut niveau pour construire et entraîner des modèles de deep learning\n",
    "from keras import layers  # Modules de couches pour construire des modèles Keras\n",
    "from keras import models  # Outils pour créer des modèles Keras\n",
    "from keras import optimizers  # Outils d'optimisation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Générateur d'images pour l'augmentation des données\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping  # Rappels pour le suivi et l'arrêt précoce\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Lambda  # Types de couches Keras\n",
    "from keras.layers import Conv2D, MaxPooling2D  # Couches convolutionnelles et de pooling\n",
    "from keras.preprocessing import image  # Outils de prétraitement d'images\n",
    "from tensorflow.keras.models import Model, load_model  # Définition / chargement de modèles\n",
    "from keras.datasets import fashion_mnist  # Jeu de données Fashion MNIST\n",
    "from tensorflow.keras.utils import to_categorical  # Conversion en encodage one-hot\n",
    "from tensorflow.keras.optimizers import SGD  # Optimiseur Stochastic Gradient Descent\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50  # Modèle ResNet50 pré-entraîné\n",
    "from tensorflow.keras.preprocessing import image  # Prétraitement d'images pour les modèles Keras\n",
    "\n",
    "def tf_cleanup(close_plots: bool = False): # clean ram sinon leaks/crashs\n",
    "    if close_plots:\n",
    "        try:\n",
    "            plt.close('all')\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        tf.keras.backend.clear_session()\n",
    "    except Exception:\n",
    "        pass\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331c7313",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c875e",
   "metadata": {},
   "source": [
    "## File declare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9713b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du répertoire cible\n",
    "data_dir = \"./data/dataset/sheep_cat_elephant_with_caption_600\"\n",
    "data_dir_img = os.path.join(data_dir, \"images\")\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384679e4",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du répertoire s'il n'existe pas\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "zip_file = \"Data_sheep_cat_elephant_with_caption_600.zip\"\n",
    "\n",
    "#!wget https://www.lirmm.fr/~poncelet/Ressources/cnn_models.zip\n",
    "!Powershell.exe -Command ((new-object System.Net.WebClient).DownloadFile('https://www.lirmm.fr/~poncelet/Ressources/Data_sheep_cat_elephant_with_caption_600.zip','Data_sheep_cat_elephant_with_caption_600.zip'))\n",
    "\n",
    "# Extraction du fichier ZIP\n",
    "with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "\n",
    "# Suppression du fichier ZIP après extraction pour économiser de l'espace\n",
    "os.remove(zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff4309",
   "metadata": {},
   "source": [
    "# GA class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcde0c65",
   "metadata": {},
   "source": [
    "Modèle avec paramètres d'archi modifiables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f629c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInstance:\n",
    "    def __init__(self, \n",
    "                 cLayers, # nombre de couches convolutionnelles\n",
    "                 cDims, # [(nb_filtres : int, taille_filtres : (int, int), taille_pooling : (int, int))]\n",
    "                 dLayers, # nombre de couches denses\n",
    "                 dDims, # [nb_neurones : int]\n",
    "                 dropout, # taux de dropout (ignoré si 0f)\n",
    "                 input_shape, # (int, int, int)\n",
    "                 output_shape, # int\n",
    "                 name\n",
    "                 ):\n",
    "        # couche d'entrée\n",
    "        input = Input(shape=input_shape, name=\"input\")\n",
    "        x = input\n",
    "        # couches conv\n",
    "        for i in range(cLayers):\n",
    "            x = Conv2D(cDims[i][0], cDims[i][1], activation=\"relu\", name=f\"conv_{i+1}_relu_{cDims[i][0]}_{cDims[i][1][0]}.{cDims[i][1][1]}\")(x)\n",
    "            x = MaxPooling2D(cDims[i][2], name=f\"pool_{i+1}_{cDims[i][2][0]}.{cDims[i][2][1]}\")(x)\n",
    "        # flatten\n",
    "        x = Flatten(name=\"flatten\")(x)\n",
    "        # couches denses\n",
    "        for i in range(dLayers):\n",
    "            x = Dense((int(dDims[i])), activation=\"relu\", name=f\"dense_{i+1}\")(x)\n",
    "        # couche de sortie\n",
    "        if dropout[0] > 0:\n",
    "            x = Dropout(dropout[0], name=f\"dropout_{dropout[0]}\")(x)\n",
    "        output = Dense(output_shape, activation=\"softmax\", name=\"output\")(x)\n",
    "        model = Model(inputs=input, outputs=output, name=name)\n",
    "        model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "        self.model = model\n",
    "        self.cLayers = cLayers\n",
    "        self.cDims = cDims\n",
    "        self.dLayers = dLayers\n",
    "        self.dDims = dDims\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.name = name\n",
    "        \n",
    "    def summary(self):\n",
    "        return self.model.summary()\n",
    "    def fit(self, x, y=None, **kwargs):\n",
    "        return self.model.fit(x, y, **kwargs)\n",
    "    def evaluate(self, x, y=None, **kwargs):\n",
    "        return self.model.evaluate(x, y, **kwargs)\n",
    "\n",
    "    def predict(self, x, **kwargs):\n",
    "        return self.model.predict(x, **kwargs)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.save(path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model = load_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37248445",
   "metadata": {},
   "source": [
    "Gene + Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "088425a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity:\n",
    "    def __init__(self,\n",
    "                 cDims,  # [(nb_filtres : int, taille_filtres : (int, int), taille_pooling : (int, int))]\n",
    "                 dDims,  # [nb_neurones : int]\n",
    "                 dropout,  # taux de dropout (ignoré si 0f)\n",
    "                 input_shape,  # (int, int, int)\n",
    "                 output_shape,  # int,\n",
    "                 name\n",
    "                 ):\n",
    "        self.cLayers = len(cDims)\n",
    "        self.cDims = cDims\n",
    "        self.dLayers = len(dDims)\n",
    "        self.dDims = dDims\n",
    "        self.dropout = dropout\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.name = name\n",
    "        self.model_instance = None\n",
    "\n",
    "        self.loss = None\n",
    "        self.accuracy = None\n",
    "        self.complexity = None\n",
    "\n",
    "        self._ensure_model()\n",
    "        self.complexity = float(self.model_instance.model.count_params())\n",
    "        self.drop_model()\n",
    "\n",
    "    def _ensure_model(self):\n",
    "        if self.model_instance is None:\n",
    "            self.model_instance = ModelInstance(\n",
    "                self.cLayers, self.cDims, self.dLayers, self.dDims, self.dropout, self.input_shape, self.output_shape, self.name\n",
    "            )\n",
    "        return self.model_instance\n",
    "\n",
    "    def drop_model(self, close_plots: bool = False):\n",
    "        try:\n",
    "            if self.model_instance is not None:\n",
    "                try:\n",
    "                    self.model_instance.model.stop_training = True # pour pouvoir completement de-alloc\n",
    "                except Exception:\n",
    "                    pass\n",
    "                self.model_instance.model = None\n",
    "        except Exception:\n",
    "            pass\n",
    "        self.model_instance = None\n",
    "        tf_cleanup(close_plots=close_plots)\n",
    "        return self\n",
    "\n",
    "    def evaluate_fitness(self, test_data):\n",
    "        self._ensure_model()\n",
    "        self.loss, self.accuracy = self.model_instance.evaluate(test_data)\n",
    "        return self.accuracy\n",
    "\n",
    "    def compute_complexity(self):\n",
    "        # trainable params \n",
    "        self._ensure_model()\n",
    "        self.complexity = float(self.model_instance.model.count_params())\n",
    "        return self.complexity\n",
    "\n",
    "    def summary(self):\n",
    "        self._ensure_model()\n",
    "        return self.model_instance.summary()\n",
    "\n",
    "    def fit(self, x, y=None, **kwargs):\n",
    "        self._ensure_model()\n",
    "        return self.model_instance.fit(x, y, **kwargs)\n",
    "\n",
    "    def evaluate(self, x, y=None, **kwargs):\n",
    "        self._ensure_model()\n",
    "        self.loss, self.accuracy = self.model_instance.evaluate(x, y, **kwargs)\n",
    "        return self.loss, self.accuracy\n",
    "\n",
    "    def predict(self, x, **kwargs):\n",
    "        self._ensure_model()\n",
    "        return self.model_instance.predict(x, **kwargs)\n",
    "\n",
    "    def reset(self):\n",
    "        self.drop_model()\n",
    "        self.model_instance = ModelInstance(\n",
    "            self.cLayers, self.cDims, self.dLayers, self.dDims, self.dropout, self.input_shape, self.output_shape, self.name\n",
    "        )\n",
    "        self.loss = None\n",
    "        self.accuracy = None\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec853b9",
   "metadata": {},
   "source": [
    "## Data + train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10fffbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir_img,\n",
    "        validation_split=0.3,\n",
    "        subset=\"training\",\n",
    "        seed=124,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        label_mode=\"int\",\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir_img,\n",
    "        validation_split=0.3,\n",
    "        subset=\"validation\",\n",
    "        seed=124,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        label_mode=\"int\",\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    X_train_list, y_train_list = [], []\n",
    "    for x, y in train_ds:\n",
    "        X_train_list.append(x.numpy())\n",
    "        y_train_list.append(y.numpy())\n",
    "\n",
    "    X_test_list, y_test_list = [], []\n",
    "    for x, y in val_ds:\n",
    "        X_test_list.append(x.numpy())\n",
    "        y_test_list.append(y.numpy())\n",
    "\n",
    "    X_train = np.concatenate(X_train_list, axis=0)\n",
    "    y_train = np.concatenate(y_train_list, axis=0)\n",
    "    X_test = np.concatenate(X_test_list, axis=0)\n",
    "    y_test = np.concatenate(y_test_list, axis=0)\n",
    "\n",
    "    # One-hot\n",
    "    numClass = len(train_ds.class_names)\n",
    "    y_train = to_categorical(y_train, num_classes=numClass)\n",
    "    y_test = to_categorical(y_test, num_classes=numClass)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8182c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(train, test):\n",
    "    \"\"\"\n",
    "    Prétraitement des données : conversion en float, normalisation entre 0 et 1.\n",
    "\n",
    "    Paramètres :\n",
    "    - train : tableau de données d'entraînement\n",
    "    - test : tableau de données de test\n",
    "\n",
    "    Retourne :\n",
    "    - train_norm : données d'entraînement normalisées\n",
    "    - test_norm : données de test normalisées\n",
    "    \"\"\"\n",
    "    # Conversion des entiers en floats pour permettre la normalisation\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "\n",
    "    # Normalisation des valeurs entre 0 et 1\n",
    "    train_norm /= 255.0\n",
    "    test_norm /= 255.0\n",
    "\n",
    "    return train_norm, test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c6832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "augment = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomTranslation(0.2, 0.2),\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    ],\n",
    "    name=\"augment\",\n",
    ")\n",
    "\n",
    "def make_train_dataset(X, y, batch_size, mult_datagen=1):\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    ds = ds.shuffle(buffer_size=len(X), reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size, drop_remainder=False)\n",
    "    ds = ds.map(lambda x, y: (augment(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.repeat().prefetch(AUTOTUNE)\n",
    "    base_steps = int(np.ceil(len(X) / batch_size))\n",
    "    steps_per_epoch = mult_datagen * base_steps\n",
    "    return ds, steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c55560f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataX, dataY, folds=5, epochs=10, keep_histories=False, use_augmentation=True, mult_datagen=1):\n",
    "    \"\"\"\n",
    "    Évalue le modèle avec une validation croisée K-fold.\n",
    "    \"\"\"\n",
    "    scores, losses = [], []\n",
    "    histories = []\n",
    "    kfold = KFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "    print(model.summary())\n",
    "\n",
    "    for train_ix, test_ix in kfold.split(dataX):\n",
    "        X_train, y_train = dataX[train_ix], dataY[train_ix]\n",
    "        X_test, y_test = dataX[test_ix], dataY[test_ix]\n",
    "\n",
    "        # Modèle neuf pour ce fold\n",
    "        model = model.reset()\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)]\n",
    "\n",
    "        if use_augmentation:\n",
    "            train_ds, steps_per_epoch = make_train_dataset(\n",
    "                X_train, y_train, batch_size=batch_size, mult_datagen=mult_datagen\n",
    "            )\n",
    "            history = model.fit(\n",
    "                train_ds,\n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                verbose=1,\n",
    "                callbacks=callbacks,\n",
    "            )\n",
    "        else:\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                verbose=1,\n",
    "                callbacks=callbacks\n",
    "            )\n",
    "\n",
    "        loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "        scores.append(float(acc))\n",
    "        losses.append(float(loss))\n",
    "        if keep_histories:\n",
    "            histories.append(history)\n",
    "\n",
    "    model.drop_model()\n",
    "\n",
    "    # Affichage des statistiques de précision : moyenne et écart-type\n",
    "    print(f'Précision : moyenne={np.mean(scores) * 100:.3f}% écart-type={std(scores) * 100:.3f}%, k={len(scores)}')\n",
    "    model.accuracy = float(np.mean(scores)) if len(scores) else None\n",
    "    model.loss = float(np.mean(losses)) if len(losses) else None\n",
    "    return scores, histories if keep_histories else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e255098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(histories):\n",
    "    \"\"\"\n",
    "    Fonction pour afficher les courbes de loss et d'accuracy\n",
    "    moyennees et ecart-types a travers les k-folds.\n",
    "\n",
    "    Parametres :\n",
    "    - histories (list) : Historique d'entrainement des differents plis K-folds.\n",
    "    \"\"\"\n",
    "    if not histories:\n",
    "        return\n",
    "\n",
    "    # Aligne les historiques sur la longueur minimale (early stopping).\n",
    "    min_len = min(len(h.history[\"loss\"]) for h in histories)\n",
    "    trimmed = []\n",
    "    for h in histories:\n",
    "        trimmed.append({k: v[:min_len] for k, v in h.history.items()})\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    epochs = range(min_len)\n",
    "\n",
    "    mean_loss = np.mean([h[\"loss\"] for h in trimmed], axis=0)\n",
    "    std_loss = np.std([h[\"loss\"] for h in trimmed], axis=0)\n",
    "    mean_val_loss = np.mean([h[\"val_loss\"] for h in trimmed], axis=0)\n",
    "    std_val_loss = np.std([h[\"val_loss\"] for h in trimmed], axis=0)\n",
    "\n",
    "    mean_accuracy = np.mean([h[\"accuracy\"] for h in trimmed], axis=0)\n",
    "    std_accuracy = np.std([h[\"accuracy\"] for h in trimmed], axis=0)\n",
    "    mean_val_accuracy = np.mean([h[\"val_accuracy\"] for h in trimmed], axis=0)\n",
    "    std_val_accuracy = np.std([h[\"val_accuracy\"] for h in trimmed], axis=0)\n",
    "\n",
    "    train_color = 'blue'\n",
    "    val_color = 'orange'\n",
    "\n",
    "    ax1.plot(epochs, mean_loss, color=train_color, label='Train')\n",
    "    ax1.fill_between(epochs, mean_loss - std_loss, mean_loss + std_loss, color=train_color, alpha=0.2)\n",
    "    ax1.plot(epochs, mean_val_loss, color=val_color, label='Validation')\n",
    "    ax1.fill_between(epochs, mean_val_loss - std_val_loss, mean_val_loss + std_val_loss, color=val_color, alpha=0.2)\n",
    "\n",
    "    ax2.plot(epochs, mean_accuracy, color=train_color, label='Train')\n",
    "    ax2.fill_between(epochs, mean_accuracy - std_accuracy, mean_accuracy + std_accuracy, color=train_color, alpha=0.2)\n",
    "    ax2.plot(epochs, mean_val_accuracy, color=val_color, label='Validation')\n",
    "    ax2.fill_between(epochs, mean_val_accuracy - std_val_accuracy, mean_val_accuracy + std_val_accuracy, color=val_color, alpha=0.2)\n",
    "\n",
    "    ax1.set_title(f'Loss (k={len(histories)})')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.set_title(f'Accuracy (k={len(histories)})')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef00ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(folds, epochs, model, X_train, y_train, X_test, y_test, plot=True):\n",
    "    print(model.summary())\n",
    "    scores, histories = evaluate_model(model, X_train, y_train, folds, epochs, keep_histories=plot)\n",
    "    if plot and histories is not None:\n",
    "        plot_curves(histories)\n",
    "    print(f'Précision : moyenne={mean(scores) * 100:.3f}% écart-type={std(scores) * 100:.3f}%, k={len(scores)}')\n",
    "    # cleanup final (histoires/figures)\n",
    "    tf_cleanup(close_plots=plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0f6d1",
   "metadata": {},
   "source": [
    "# GA RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b70a8",
   "metadata": {},
   "source": [
    "## params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa62e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Gene\n",
    "input_shape = (img_height, img_width, 3)\n",
    "cDims = [\n",
    "    (2, (3, 3), (2, 2)),\n",
    "    (4, (5, 5), (2, 2)),\n",
    "    (6, (3, 3), (2, 2)),\n",
    "    (8, (3, 3), (2, 2))\n",
    "    ]\n",
    "dDims = [20]\n",
    "dropout = [0.5]\n",
    "output_shape = 3\n",
    "\n",
    "\n",
    "# Pop params\n",
    "num_pop = 5\n",
    "num_gen = 5\n",
    "\n",
    "# params\n",
    "epochs = 40\n",
    "folds = 3\n",
    "\n",
    "# si augment\n",
    "mult_datagen = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cac288a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isArchitectureValid(inputDim, outputDim, cDims, dDims, dropout):  # Verifie si la mutation fera crash\n",
    "\n",
    "    try:\n",
    "        h = inputDim[0]\n",
    "        w = inputDim[1]\n",
    "        for (filters, kernel, pool) in cDims:\n",
    "            kh, kw = int(kernel[0]), int(kernel[1])\n",
    "            ph, pw = int(pool[0]), int(pool[1])\n",
    "            # Conv2D padding valid? => output = input - kernel + 1\n",
    "            h = h - kh + 1\n",
    "            w = w - kw + 1\n",
    "            if h <= 0 or w <= 0:\n",
    "                return False\n",
    "            # MaxPooling2D padding valid? => floor division\n",
    "            h = h // ph\n",
    "            w = w // pw\n",
    "            if h <= 0 or w <= 0:\n",
    "                return False\n",
    "        for i in range(len(cDims)-1):\n",
    "            if cDims[i+1][0] < cDims[i][0]:\n",
    "                return False\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50c4affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(conv, cDims, dense, dDims, dropout):\n",
    "    taille_filtres = [(3, 3), (5, 5), (7, 7)]\n",
    "    nb_filtres = [2, 4, 6, 8, 16, 32, 64, 128]\n",
    "    taille_pooling = [(1, 1), (2, 2), (3, 3)]\n",
    "    neur_dense = [5, 10, 15, 20]\n",
    "    dropout_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    d = dense\n",
    "    c = conv\n",
    "    mutation_type = np.random.randint(1, 8)\n",
    "    if mutation_type == 1: # add conv layer\n",
    "        if c < 6:\n",
    "            c += 1\n",
    "            cDims.append((nb_filtres[-1], taille_filtres[0], taille_pooling[0]))\n",
    "        else:\n",
    "            return mutate(c, cDims, d, dDims, dropout)\n",
    "    elif mutation_type == 2: # pop conv layer\n",
    "        if c > 2:\n",
    "            c -= 1\n",
    "            cDims.pop()\n",
    "        else:\n",
    "            return mutate(c, cDims, d, dDims, dropout)\n",
    "    elif mutation_type == 3: # change a conv layer param\n",
    "        layer_idx = np.random.randint(0, c)\n",
    "        param_idx = np.random.randint(0, 3)\n",
    "        if param_idx == 0: # change nb_filtres\n",
    "            cDims[layer_idx] = (np.random.choice(nb_filtres), cDims[layer_idx][1], cDims[layer_idx][2])\n",
    "        elif param_idx == 1: # change taille_filtres\n",
    "            cDims[layer_idx] = (cDims[layer_idx][0], taille_filtres[np.random.randint(0, len(taille_filtres))], cDims[layer_idx][2])\n",
    "        else: # change taille_pooling\n",
    "            cDims[layer_idx] = (cDims[layer_idx][0], cDims[layer_idx][1], taille_pooling[np.random.randint(0, len(taille_pooling))])\n",
    "    elif mutation_type == 4: # add dense layer\n",
    "        if d < 2:\n",
    "            d += 1\n",
    "            dDims.append(neur_dense[0])\n",
    "        else:\n",
    "            return mutate(c, cDims, d, dDims, dropout)\n",
    "    elif mutation_type == 5: # pop dense\n",
    "        if d > 1:\n",
    "            d -= 1\n",
    "            dDims.pop()\n",
    "    elif mutation_type == 6: # change dropout\n",
    "        dropout = [np.random.choice(dropout_values)]\n",
    "    else: # change a dense param\n",
    "        if d > 0:\n",
    "            layer_idx = np.random.randint(0, d)\n",
    "            dDims[layer_idx] = np.random.choice(neur_dense)\n",
    "        else:\n",
    "            return mutate(c, cDims, d, dDims, dropout)\n",
    "    return c, cDims, d, dDims, dropout\n",
    "\n",
    "\n",
    "def mutate_unique(cDims, dDims, existing_configs, dropout):\n",
    "    new_config = mutate(len(cDims), cDims.copy(), len(dDims), dDims.copy(), dropout.copy())\n",
    "    valid = True\n",
    "    for config in existing_configs:\n",
    "        conv_match = False\n",
    "        cDims_match = True\n",
    "        if new_config[0] == config[0]: # conv\n",
    "            conv_match = True\n",
    "            for i in range(len(new_config[1])):\n",
    "                if new_config[1][i] != config[1][i]: # cDims\n",
    "                    cDims_match = False\n",
    "        dense_match = False\n",
    "        dDims_match = True\n",
    "        if new_config[2] == config[2]: # dense\n",
    "            dense_match = True\n",
    "            for i in range(len(new_config[3])):\n",
    "                if new_config[3][i] != config[3][i]: # dDims\n",
    "                    dDims_match = False\n",
    "        dropout_match = False\n",
    "        if new_config[4] == config[4]: # dropout\n",
    "                dropout_match = True\n",
    "        if conv_match and cDims_match and dense_match and dDims_match and dropout_match:\n",
    "            valid = False\n",
    "            break\n",
    "    if valid and isArchitectureValid(input_shape, output_shape, cDims, dDims, dropout):\n",
    "        existing_configs.append(new_config)\n",
    "        return new_config[1], new_config[3], new_config[4]\n",
    "    else:\n",
    "        return mutate_unique(cDims, dDims, existing_configs, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42d58af",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "470380f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1800 files belonging to 3 classes.\n",
      "Using 1260 files for training.\n",
      "Found 1800 files belonging to 3 classes.\n",
      "Using 540 files for validation.\n",
      "Generation 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"0_0\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"0_0\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_1_relu_2_3.3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool_1_2.2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_2_relu_4_5.5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">107</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">107</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">204</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool_2_2.2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_3_relu_6_3.3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool_3_2.2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_4_relu_8_3.3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">440</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool_4_2.2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">968</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,380</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_0.5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_1_relu_2_3.3 (\u001b[38;5;33mConv2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m2\u001b[0m)    │            \u001b[38;5;34m56\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool_1_2.2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m2\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_2_relu_4_5.5 (\u001b[38;5;33mConv2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m107\u001b[0m, \u001b[38;5;34m107\u001b[0m, \u001b[38;5;34m4\u001b[0m)    │           \u001b[38;5;34m204\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool_2_2.2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m4\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_3_relu_6_3.3 (\u001b[38;5;33mConv2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m51\u001b[0m, \u001b[38;5;34m51\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │           \u001b[38;5;34m222\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool_3_2.2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_4_relu_8_3.3 (\u001b[38;5;33mConv2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │           \u001b[38;5;34m440\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool_4_2.2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m968\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │        \u001b[38;5;34m19,380\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_0.5 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m63\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,365</span> (79.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,365\u001b[0m (79.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,365</span> (79.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,365\u001b[0m (79.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/40\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.3838 - loss: 1.0897 - val_accuracy: 0.5048 - val_loss: 1.0431\n",
      "Epoch 2/40\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.4679 - loss: 1.0321 - val_accuracy: 0.5000 - val_loss: 1.0196\n",
      "Epoch 3/40\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5000 - loss: 1.0144 - val_accuracy: 0.5690 - val_loss: 0.9618\n",
      "Epoch 4/40\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5267 - loss: 0.9842 - val_accuracy: 0.6024 - val_loss: 0.9107\n",
      "Epoch 5/40\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5429 - loss: 0.9589 - val_accuracy: 0.6119 - val_loss: 0.8771\n",
      "Epoch 6/40\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5455 - loss: 0.9470 - val_accuracy: 0.5762 - val_loss: 0.9049\n",
      "Epoch 7/40\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5652 - loss: 0.9214 - val_accuracy: 0.6738 - val_loss: 0.8165\n",
      "Epoch 8/40\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5952 - loss: 0.8941 - val_accuracy: 0.6595 - val_loss: 0.8165\n",
      "Epoch 9/40\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5948 - loss: 0.8905 - val_accuracy: 0.6929 - val_loss: 0.7476\n",
      "Epoch 10/40\n",
      "\u001b[1m45/70\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5963 - loss: 0.8746"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m entity \u001b[38;5;129;01mis\u001b[39;00m prev_best \u001b[38;5;129;01mand\u001b[39;00m baseline_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_histories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_augmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmult_datagen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmult_datagen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Cleanup par individu\u001b[39;00m\n\u001b[0;32m     31\u001b[0m entity\u001b[38;5;241m.\u001b[39mdrop_model()\n",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, dataX, dataY, folds, epochs, keep_histories, use_augmentation, mult_datagen)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_augmentation:\n\u001b[0;32m     19\u001b[0m     train_ds, steps_per_epoch \u001b[38;5;241m=\u001b[39m make_train_dataset(\n\u001b[0;32m     20\u001b[0m         X_train, y_train, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, mult_datagen\u001b[38;5;241m=\u001b[39mmult_datagen\n\u001b[0;32m     21\u001b[0m     )\n\u001b[1;32m---> 22\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     32\u001b[0m         X_train, y_train,\n\u001b[0;32m     33\u001b[0m         validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks\n\u001b[0;32m     38\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[4], line 66\u001b[0m, in \u001b[0;36mEntity.fit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_model()\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_instance\u001b[38;5;241m.\u001b[39mfit(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[3], line 42\u001b[0m, in \u001b[0;36mModelInstance.fit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\theen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\theen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:399\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    398\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[1;32m--> 399\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\theen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:241\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    239\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    240\u001b[0m     ):\n\u001b[1;32m--> 241\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    243\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\theen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\theen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\theen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\theen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\theen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\theen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\theen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\theen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\theen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Chargement du jeu de données d'entraînement et de test\n",
    "X_train, y_train, X_test, y_test = load_dataset()\n",
    "\n",
    "# Prétraitement des données : nettoyage et normalisation\n",
    "X_train, X_test = clean_data(X_train, X_test)\n",
    "\n",
    "pop = [Entity(cDims, dDims, dropout.copy(), input_shape, output_shape, \"0_0\")]  # base pour controle\n",
    "conf = [[len(cDims), cDims.copy(), len(dDims), dDims.copy(), dropout.copy()]]\n",
    "for i in range(num_pop - 1):\n",
    "    pop.append(Entity(*mutate_unique(cDims.copy(), dDims.copy(), conf, dropout), input_shape, output_shape, \"0_\"+str(i+1)))\n",
    "for i in range(num_gen):\n",
    "    # ENTRAINEMENT + EVAL\n",
    "\n",
    "    print(f\"Generation {i+1}\")\n",
    "    prev_best = pop[0]\n",
    "    baseline_loss = prev_best.loss\n",
    "    for entity in pop:\n",
    "        # Un seul entrainement pour le meilleur precedent\n",
    "        if entity is prev_best and baseline_loss is not None:\n",
    "            continue\n",
    "        evaluate_model(\n",
    "            entity,\n",
    "            X_train, y_train,\n",
    "            folds=folds,\n",
    "            epochs=epochs,\n",
    "            keep_histories=False,\n",
    "            use_augmentation=True,\n",
    "            mult_datagen=mult_datagen,\n",
    "        )\n",
    "        # Cleanup par individu\n",
    "        entity.drop_model()\n",
    "\n",
    "    # Clean RAM (génération)\n",
    "    tf_cleanup(close_plots=True)\n",
    "\n",
    "    # SELECTION (A AMELIORER)\n",
    "    if baseline_loss is None:\n",
    "        baseline_loss = pop[0].loss\n",
    "    pop = [entity for entity in pop if entity.loss < baseline_loss]\n",
    "    if (len(pop) == 0): # si rien est strictement meilleur, on garde prev_best \n",
    "        pop = [prev_best]\n",
    "    # on trie par simplicite\n",
    "    pop.sort(key=lambda x: x.complexity)\n",
    "    # on choisi le plus simple\n",
    "    best_entities = [pop[0]]\n",
    "\n",
    "    print(\"Best entity :\")\n",
    "    print(f\"Loss : {best_entities[0].loss:.3f}, Complexity : {best_entities[0].complexity:.3f}\")\n",
    "    print(best_entities[0].summary())\n",
    "    print(f\"param best: conv={best_entities[0].cLayers}, cDims={best_entities[0].cDims}, dense={best_entities[0].dLayers}, dDims={best_entities[0].dDims}, dropout={best_entities[0].dropout}\")\n",
    "\n",
    "    best_entities[0].drop_model()\n",
    "\n",
    "    # MUTATRON !!!!\n",
    "\n",
    "    # generer new pop en mutant la best\n",
    "    pop = [best_entities[0]]\n",
    "    conf = [[best_entities[0].cLayers, best_entities[0].cDims.copy(), best_entities[0].dLayers, best_entities[0].dDims.copy(), best_entities[0].dropout.copy()]]\n",
    "    for j in range(num_pop - 1):\n",
    "        pop.append(Entity(\n",
    "            *mutate_unique(\n",
    "                best_entities[0].cDims.copy(),\n",
    "                best_entities[0].dDims.copy(),\n",
    "                conf,\n",
    "                best_entities[0].dropout.copy()\n",
    "            ),\n",
    "            input_shape,\n",
    "            output_shape,\n",
    "            str(i+1)+\"_\"+str(j+1)\n",
    "        ))\n",
    "\n",
    "print(\"Final best entity :\")\n",
    "print(f\"loss : {best_entities[0].loss:.3f}, Complexity : {best_entities[0].complexity:.3f}\")\n",
    "print(best_entities[0].summary())\n",
    "print(f\"param best: conv={best_entities[0].cLayers}, cDims={best_entities[0].cDims}, dense={best_entities[0].dLayers}, dDims={best_entities[0].dDims}, dropout={best_entities[0].dropout}\")\n",
    "best_entities[0].drop_model()\n",
    "tf_cleanup(close_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c33d84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
