{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f31b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install (versions compatibles + évite le package `keras` standalone)\n",
    "%pip install -U pip setuptools wheel\n",
    "%pip install -U tabulate scikit-learn pandas numpy matplotlib seaborn gensim \"tensorflow==2.16.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f715342",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/2007.12673 - Genetic Algorithm: Reviews, Implementations, and Applications - Tanweer Alam, Shamimul Qamar, Amit Dixit, Mohamed Benaida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726cd6d",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des avertissements liés à Scikit-learn\n",
    "import warnings  # Masquer les avertissements (ex. : FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import gc  # garbage collector (32Gb suffit pas)\n",
    "\n",
    "# Librairies générales\n",
    "import pandas as pd  # Librairie pour la manipulation de données\n",
    "import numpy as np  # Librairie pour le calcul numérique\n",
    "import sys  # Fonctions et variables liées à l'interpréteur Python\n",
    "import copy  # Création de copies d'objets\n",
    "from numpy import mean, std  # Fonctions de calcul de moyenne et d'écart type\n",
    "import zipfile  # Traitement de fichiers zip\n",
    "import os  # Manipulation de fichiers et chemins\n",
    "\n",
    "# Librairie affichage\n",
    "import matplotlib.pyplot as plt  # Outils de visualisation 2D\n",
    "from matplotlib import pyplot  # Interface de la bibliothèque Matplotlib\n",
    "import seaborn as sns  # Bibliothèque de visualisation de données basée sur Matplotlib\n",
    "\n",
    "# Scikit-learn pour l'évaluation des modèles\n",
    "from sklearn.metrics import confusion_matrix  # Matrice de confusion\n",
    "from sklearn.model_selection import KFold  # Outils de validation croisée\n",
    "from sklearn.metrics import accuracy_score  # Calcul de l'accuracy\n",
    "from sklearn.model_selection import train_test_split  # Découpage train/test\n",
    "\n",
    "# TensorFlow et Keras\n",
    "import tensorflow as tf  # Librairie de deep learning\n",
    "import keras  # API haut niveau pour construire et entraîner des modèles de deep learning\n",
    "from keras import layers  # Modules de couches pour construire des modèles Keras\n",
    "from keras import models  # Outils pour créer des modèles Keras\n",
    "from keras import optimizers  # Outils d'optimisation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Générateur d'images pour l'augmentation des données\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping  # Rappels pour le suivi et l'arrêt précoce\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Lambda  # Types de couches Keras\n",
    "from keras.layers import Conv2D, MaxPooling2D  # Couches convolutionnelles et de pooling\n",
    "from keras.preprocessing import image  # Outils de prétraitement d'images\n",
    "from tensorflow.keras.models import Model, load_model  # Définition / chargement de modèles\n",
    "from keras.datasets import fashion_mnist  # Jeu de données Fashion MNIST\n",
    "from tensorflow.keras.utils import to_categorical  # Conversion en encodage one-hot\n",
    "from tensorflow.keras.optimizers import SGD  # Optimiseur Stochastic Gradient Descent\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50  # Modèle ResNet50 pré-entraîné\n",
    "from tensorflow.keras.preprocessing import image  # Prétraitement d'images pour les modèles Keras\n",
    "\n",
    "def tf_cleanup(close_plots: bool = False):\n",
    "    \"\"\"Nettoyage agressif TF/GC entre folds/individus pour limiter l'accumulation.\"\"\"\n",
    "    if close_plots:\n",
    "        try:\n",
    "            plt.close('all')\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        tf.keras.backend.clear_session()\n",
    "    except Exception:\n",
    "        pass\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331c7313",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c875e",
   "metadata": {},
   "source": [
    "## File declare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9713b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du répertoire cible\n",
    "data_dir = \"./data/dataset/sheep_cat_elephant_with_caption_600\"\n",
    "data_dir_img = os.path.join(data_dir, \"images\")\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384679e4",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du répertoire s'il n'existe pas\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "zip_file = \"Data_sheep_cat_elephant_with_caption_600.zip\"\n",
    "\n",
    "#!wget https://www.lirmm.fr/~poncelet/Ressources/cnn_models.zip\n",
    "!Powershell.exe -Command ((new-object System.Net.WebClient).DownloadFile('https://www.lirmm.fr/~poncelet/Ressources/Data_sheep_cat_elephant_with_caption_600.zip','Data_sheep_cat_elephant_with_caption_600.zip'))\n",
    "\n",
    "# Extraction du fichier ZIP\n",
    "with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "\n",
    "# Suppression du fichier ZIP après extraction pour économiser de l'espace\n",
    "os.remove(zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff4309",
   "metadata": {},
   "source": [
    "# GA class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcde0c65",
   "metadata": {},
   "source": [
    "Modèle avec paramètres d'archi modifiables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f629c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInstance:\n",
    "    def __init__(self, \n",
    "                 cLayers, # nombre de couches convolutionnelles\n",
    "                 cDims, # [(nb_filtres : int, taille_filtres : (int, int), taille_pooling : (int, int))]\n",
    "                 dLayers, # nombre de couches denses\n",
    "                 dDims, # [nb_neurones : int]\n",
    "                 dropout, # taux de dropout (ignoré si 0f)\n",
    "                 input_shape, # (int, int, int)\n",
    "                 output_shape, # int\n",
    "                 name\n",
    "                 ):\n",
    "        # couche d'entrée\n",
    "        input = Input(shape=input_shape, name=\"input\")\n",
    "        x = input\n",
    "        # couches conv\n",
    "        for i in range(cLayers):\n",
    "            x = Conv2D(cDims[i][0], cDims[i][1], activation=\"relu\", name=f\"conv_{i+1}\")(x)\n",
    "            x = MaxPooling2D(cDims[i][2], name=f\"pool_{i+1}\")(x)\n",
    "        # flatten\n",
    "        x = Flatten(name=\"flatten\")(x)\n",
    "        # couches denses\n",
    "        for i in range(dLayers):\n",
    "            x = Dense((int(dDims[i])), activation=\"relu\", name=f\"dense_{i+1}\")(x)\n",
    "        # couche de sortie\n",
    "        if dropout[0] > 0:\n",
    "            x = Dropout(dropout[0], name=\"dropout\")(x)\n",
    "        output = Dense(output_shape, activation=\"softmax\", name=\"output\")(x)\n",
    "        model = Model(inputs=input, outputs=output, name=name)\n",
    "        model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "        self.model = model\n",
    "        self.cLayers = cLayers\n",
    "        self.cDims = cDims\n",
    "        self.dLayers = dLayers\n",
    "        self.dDims = dDims\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.name = name\n",
    "    def summary(self):\n",
    "        return self.model.summary()\n",
    "    def fit(self, x, y=None, **kwargs):\n",
    "        return self.model.fit(x, y, **kwargs)\n",
    "    def evaluate(self, x, y=None, **kwargs):\n",
    "        return self.model.evaluate(x, y, **kwargs)\n",
    "\n",
    "    def predict(self, x, **kwargs):\n",
    "        return self.model.predict(x, **kwargs)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.save(path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model = load_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37248445",
   "metadata": {},
   "source": [
    "Gene + Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088425a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity:\n",
    "    def __init__(self,\n",
    "                 cDims,  # [(nb_filtres : int, taille_filtres : (int, int), taille_pooling : (int, int))]\n",
    "                 dDims,  # [nb_neurones : int]\n",
    "                 dropout,  # taux de dropout (ignoré si 0f)\n",
    "                 input_shape,  # (int, int, int)\n",
    "                 output_shape,  # int,\n",
    "                 name\n",
    "                 ):\n",
    "        self.cLayers = len(cDims)\n",
    "        self.cDims = cDims\n",
    "        self.dLayers = len(dDims)\n",
    "        self.dDims = dDims\n",
    "        self.dropout = dropout\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.name = name\n",
    "\n",
    "        # IMPORTANT: ne pas garder de modèle TF en mémoire entre générations.\n",
    "        # On ne conserve que les gènes + métriques; le modèle est (re)construit à la demande.\n",
    "        self.model_instance = None\n",
    "\n",
    "        self.loss = None\n",
    "        self.accuracy = None\n",
    "        self.complexity = None\n",
    "\n",
    "        # Complexité calculée 1 fois, sans conserver le modèle.\n",
    "        self._ensure_model()\n",
    "        self.complexity = float(self.model_instance.model.count_params())\n",
    "        self.drop_model()\n",
    "\n",
    "    def _ensure_model(self):\n",
    "        if self.model_instance is None:\n",
    "            self.model_instance = ModelInstance(\n",
    "                self.cLayers, self.cDims, self.dLayers, self.dDims, self.dropout, self.input_shape, self.output_shape, self.name\n",
    "            )\n",
    "        return self.model_instance\n",
    "\n",
    "    def drop_model(self, close_plots: bool = False):\n",
    "        \"\"\"Libère explicitement les ressources TF/Keras associées au modèle de l'entité.\"\"\"\n",
    "        try:\n",
    "            if self.model_instance is not None:\n",
    "                # casse les refs au graph/weights\n",
    "                try:\n",
    "                    self.model_instance.model.stop_training = True\n",
    "                except Exception:\n",
    "                    pass\n",
    "                self.model_instance.model = None\n",
    "        except Exception:\n",
    "            pass\n",
    "        self.model_instance = None\n",
    "        tf_cleanup(close_plots=close_plots)\n",
    "        return self\n",
    "\n",
    "    def evaluate_fitness(self, test_data):\n",
    "        self._ensure_model()\n",
    "        self.loss, self.accuracy = self.model_instance.evaluate(test_data)\n",
    "        return self.accuracy\n",
    "\n",
    "    def compute_complexity(self):\n",
    "        # trainable params sans l'optimizer; calculé quand un modèle existe\n",
    "        self._ensure_model()\n",
    "        self.complexity = float(self.model_instance.model.count_params())\n",
    "        return self.complexity\n",
    "\n",
    "    def summary(self):\n",
    "        self._ensure_model()\n",
    "        return self.model_instance.summary()\n",
    "\n",
    "    def fit(self, x, y=None, **kwargs):\n",
    "        self._ensure_model()\n",
    "        return self.model_instance.fit(x, y, **kwargs)\n",
    "\n",
    "    def evaluate(self, x, y=None, **kwargs):\n",
    "        self._ensure_model()\n",
    "        self.loss, self.accuracy = self.model_instance.evaluate(x, y, **kwargs)\n",
    "        return self.loss, self.accuracy\n",
    "\n",
    "    def predict(self, x, **kwargs):\n",
    "        self._ensure_model()\n",
    "        return self.model_instance.predict(x, **kwargs)\n",
    "\n",
    "    def reset(self):\n",
    "        # reset = reconstruire un modèle neuf (utile en k-fold)\n",
    "        self.drop_model()\n",
    "        self.model_instance = ModelInstance(\n",
    "            self.cLayers, self.cDims, self.dLayers, self.dDims, self.dropout, self.input_shape, self.output_shape, self.name\n",
    "        )\n",
    "        self.loss = None\n",
    "        self.accuracy = None\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec853b9",
   "metadata": {},
   "source": [
    "## Data + train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fffbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir_img,\n",
    "        validation_split=0.3,\n",
    "        subset=\"training\",\n",
    "        seed=124,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        label_mode=\"int\",\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir_img,\n",
    "        validation_split=0.3,\n",
    "        subset=\"validation\",\n",
    "        seed=124,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        label_mode=\"int\",\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    X_train_list, y_train_list = [], []\n",
    "    for x, y in train_ds:\n",
    "        X_train_list.append(x.numpy())\n",
    "        y_train_list.append(y.numpy())\n",
    "\n",
    "    X_test_list, y_test_list = [], []\n",
    "    for x, y in val_ds:\n",
    "        X_test_list.append(x.numpy())\n",
    "        y_test_list.append(y.numpy())\n",
    "\n",
    "    X_train = np.concatenate(X_train_list, axis=0)\n",
    "    y_train = np.concatenate(y_train_list, axis=0)\n",
    "    X_test = np.concatenate(X_test_list, axis=0)\n",
    "    y_test = np.concatenate(y_test_list, axis=0)\n",
    "\n",
    "    # One-hot\n",
    "    numClass = len(train_ds.class_names)\n",
    "    y_train = to_categorical(y_train, num_classes=numClass)\n",
    "    y_test = to_categorical(y_test, num_classes=numClass)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8182c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(train, test):\n",
    "    \"\"\"\n",
    "    Prétraitement des données : conversion en float, normalisation entre 0 et 1.\n",
    "\n",
    "    Paramètres :\n",
    "    - train : tableau de données d'entraînement\n",
    "    - test : tableau de données de test\n",
    "\n",
    "    Retourne :\n",
    "    - train_norm : données d'entraînement normalisées\n",
    "    - test_norm : données de test normalisées\n",
    "    \"\"\"\n",
    "    # Conversion des entiers en floats pour permettre la normalisation\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "\n",
    "    # Normalisation des valeurs entre 0 et 1\n",
    "    train_norm /= 255.0\n",
    "    test_norm /= 255.0\n",
    "\n",
    "    return train_norm, test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55560f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataX, dataY, folds=5, epochs=10, keep_histories=False):\n",
    "    \"\"\"\n",
    "    Évalue le modèle avec une validation croisée K-fold.\n",
    "\n",
    "    Notes mémoire:\n",
    "    - Un `Model` TF doit être détruit entre folds pour éviter l'accumulation de graph/optimizers.\n",
    "    - On évite de conserver `History` en GA (keep_histories=False).\n",
    "    \"\"\"\n",
    "    scores, losses = [], []\n",
    "    histories = []\n",
    "    kfold = KFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "    print(model.summary())\n",
    "\n",
    "    for train_ix, test_ix in kfold.split(dataX):\n",
    "        X_train, y_train = dataX[train_ix], dataY[train_ix]\n",
    "        X_test, y_test = dataX[test_ix], dataY[test_ix]\n",
    "\n",
    "        # Modèle neuf pour ce fold\n",
    "        model = model.reset()\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0)]\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "        scores.append(float(acc))\n",
    "        losses.append(float(loss))\n",
    "        if keep_histories:\n",
    "            histories.append(history)\n",
    "\n",
    "    model.drop_model()\n",
    "\n",
    "    # Affichage des statistiques de précision : moyenne et écart-type\n",
    "    print(f'Précision : moyenne={np.mean(scores) * 100:.3f}% écart-type={std(scores) * 100:.3f}%, k={len(scores)}')\n",
    "    model.accuracy = float(np.mean(scores)) if len(scores) else None\n",
    "    model.loss = float(np.mean(losses)) if len(losses) else None\n",
    "    # complexity est stable (calculée à l'init), pas besoin de rebuild ici\n",
    "    return scores, histories if keep_histories else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e255098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(histories):\n",
    "    \"\"\"\n",
    "    Fonction pour afficher les courbes de loss et d'accuracy\n",
    "    moyennees et ecart-types a travers les k-folds.\n",
    "\n",
    "    Parametres :\n",
    "    - histories (list) : Historique d'entrainement des differents plis K-folds.\n",
    "    \"\"\"\n",
    "    if not histories:\n",
    "        return\n",
    "\n",
    "    # Aligne les historiques sur la longueur minimale (early stopping).\n",
    "    min_len = min(len(h.history[\"loss\"]) for h in histories)\n",
    "    trimmed = []\n",
    "    for h in histories:\n",
    "        trimmed.append({k: v[:min_len] for k, v in h.history.items()})\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    epochs = range(min_len)\n",
    "\n",
    "    mean_loss = np.mean([h[\"loss\"] for h in trimmed], axis=0)\n",
    "    std_loss = np.std([h[\"loss\"] for h in trimmed], axis=0)\n",
    "    mean_val_loss = np.mean([h[\"val_loss\"] for h in trimmed], axis=0)\n",
    "    std_val_loss = np.std([h[\"val_loss\"] for h in trimmed], axis=0)\n",
    "\n",
    "    mean_accuracy = np.mean([h[\"accuracy\"] for h in trimmed], axis=0)\n",
    "    std_accuracy = np.std([h[\"accuracy\"] for h in trimmed], axis=0)\n",
    "    mean_val_accuracy = np.mean([h[\"val_accuracy\"] for h in trimmed], axis=0)\n",
    "    std_val_accuracy = np.std([h[\"val_accuracy\"] for h in trimmed], axis=0)\n",
    "\n",
    "    train_color = 'blue'\n",
    "    val_color = 'orange'\n",
    "\n",
    "    ax1.plot(epochs, mean_loss, color=train_color, label='Train')\n",
    "    ax1.fill_between(epochs, mean_loss - std_loss, mean_loss + std_loss, color=train_color, alpha=0.2)\n",
    "    ax1.plot(epochs, mean_val_loss, color=val_color, label='Validation')\n",
    "    ax1.fill_between(epochs, mean_val_loss - std_val_loss, mean_val_loss + std_val_loss, color=val_color, alpha=0.2)\n",
    "\n",
    "    ax2.plot(epochs, mean_accuracy, color=train_color, label='Train')\n",
    "    ax2.fill_between(epochs, mean_accuracy - std_accuracy, mean_accuracy + std_accuracy, color=train_color, alpha=0.2)\n",
    "    ax2.plot(epochs, mean_val_accuracy, color=val_color, label='Validation')\n",
    "    ax2.fill_between(epochs, mean_val_accuracy - std_val_accuracy, mean_val_accuracy + std_val_accuracy, color=val_color, alpha=0.2)\n",
    "\n",
    "    ax1.set_title(f'Loss (k={len(histories)})')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.set_title(f'Accuracy (k={len(histories)})')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(folds, epochs, model, X_train, y_train, X_test, y_test, plot=True):\n",
    "    \"\"\"Évaluation interactive (debug) : peut afficher les courbes. Pour le GA, éviter plot=True.\"\"\"\n",
    "    print(model.summary())\n",
    "    scores, histories = evaluate_model(model, X_train, y_train, folds, epochs, keep_histories=plot)\n",
    "    if plot and histories is not None:\n",
    "        plot_curves(histories)\n",
    "    print(f'Précision : moyenne={mean(scores) * 100:.3f}% écart-type={std(scores) * 100:.3f}%, k={len(scores)}')\n",
    "    # cleanup final (histoires/figures)\n",
    "    tf_cleanup(close_plots=plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0f6d1",
   "metadata": {},
   "source": [
    "# GA RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b70a8",
   "metadata": {},
   "source": [
    "## params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa62e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Gene\n",
    "input_shape = (img_height, img_width, 3)\n",
    "cDims = [\n",
    "    (6, (3, 3), (2, 2)),\n",
    "    (32, (5, 5), (2, 2)),\n",
    "    (6, (3, 3), (2, 2))\n",
    "    ]\n",
    "dDims = [20]\n",
    "dropout = [0.0]\n",
    "output_shape = 3\n",
    "epochs = 20\n",
    "folds = 10\n",
    "# choices :\n",
    "# 1. Add conv layer\n",
    "# 2. pop conv layer\n",
    "# 3. change a conv layer param\n",
    "#   3.1 change nb_filtres\n",
    "#   3.2 change taille_filtres\n",
    "#   3.3 change taille_pooling\n",
    "# 4. add dense layer\n",
    "# 5. pop dense layer\n",
    "# 6. change a dense layer param\n",
    "#  6.1 change nb_neurones\n",
    "\n",
    "# Pop params\n",
    "num_pop = 20\n",
    "num_gen = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac288a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isArchitectureValid(inputDim, outputDim, cDims, dDims, dropout):  # Verifie si la mutation fera crash\n",
    "\n",
    "    try:\n",
    "        h = inputDim[0]\n",
    "        w = inputDim[1]\n",
    "        for (filters, kernel, pool) in cDims:\n",
    "            kh, kw = int(kernel[0]), int(kernel[1])\n",
    "            ph, pw = int(pool[0]), int(pool[1])\n",
    "            # Conv2D padding valid? => output = input - kernel + 1\n",
    "            h = h - kh + 1\n",
    "            w = w - kw + 1\n",
    "            if h <= 0 or w <= 0:\n",
    "                return False\n",
    "            # MaxPooling2D padding valid? => floor division\n",
    "            h = h // ph\n",
    "            w = w // pw\n",
    "            if h <= 0 or w <= 0:\n",
    "                return False\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c4affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(conv, cDims, dense, dDims, dropout):\n",
    "    taille_filtres = [(3, 3), (5, 5), (7, 7)]\n",
    "    nb_filtres = [6, 8, 16, 32, 64, 128]\n",
    "    taille_pooling = [(2, 2), (3, 3)]\n",
    "    neur_dense = [10, 20, 50, 100, 150, 200]\n",
    "    dropout_values = [0.0, 0.5, 0.75]\n",
    "    d = dense\n",
    "    c = conv\n",
    "    mutation_type = np.random.randint(1, 8)\n",
    "    if mutation_type == 1: # add conv layer\n",
    "        if c < 6:\n",
    "            c += 1\n",
    "            cDims.append((nb_filtres[0], taille_filtres[0], taille_pooling[0]))\n",
    "        else:\n",
    "            return mutate(c, cDims, d, dDims, dropout)\n",
    "    elif mutation_type == 2: # pop conv layer\n",
    "        if c > 2:\n",
    "            c -= 1\n",
    "            cDims.pop()\n",
    "        else:\n",
    "            return mutate(c, cDims, d, dDims, dropout)\n",
    "    elif mutation_type == 3: # change a conv layer param\n",
    "        layer_idx = np.random.randint(0, c)\n",
    "        param_idx = np.random.randint(0, 3)\n",
    "        if param_idx == 0: # change nb_filtres\n",
    "            cDims[layer_idx] = (np.random.choice(nb_filtres), cDims[layer_idx][1], cDims[layer_idx][2])\n",
    "        elif param_idx == 1: # change taille_filtres\n",
    "            cDims[layer_idx] = (cDims[layer_idx][0], taille_filtres[np.random.randint(0, len(taille_filtres))], cDims[layer_idx][2])\n",
    "        else: # change taille_pooling\n",
    "            cDims[layer_idx] = (cDims[layer_idx][0], cDims[layer_idx][1], taille_pooling[np.random.randint(0, len(taille_pooling))])\n",
    "    elif mutation_type == 4: # add dense layer\n",
    "        if d < 2:\n",
    "            d += 1\n",
    "            dDims.append(neur_dense[0])\n",
    "        else:\n",
    "            return mutate(c, cDims, d, dDims, dropout)\n",
    "    elif mutation_type == 5: # pop dense\n",
    "        if d > 1:\n",
    "            d -= 1\n",
    "            dDims.pop()\n",
    "    elif mutation_type == 6: # change dropout\n",
    "        dropout = [np.random.choice(dropout_values)]\n",
    "    else: # change a dense param\n",
    "        if d > 0:\n",
    "            layer_idx = np.random.randint(0, d)\n",
    "            dDims[layer_idx] = np.random.choice(neur_dense)\n",
    "        else:\n",
    "            return mutate(c, cDims, d, dDims, dropout)\n",
    "    return c, cDims, d, dDims, dropout\n",
    "\n",
    "\n",
    "def mutate_unique(cDims, dDims, existing_configs, dropout):\n",
    "    for _ in range(10):  # Limite le nombre de tentatives pour trouver une mutation unique\n",
    "        new_config = mutate(len(cDims), cDims, len(dDims), dDims, dropout)\n",
    "        valid = True\n",
    "        for config in existing_configs:\n",
    "            conv_match = False\n",
    "            cDims_match = True\n",
    "            if new_config[0] == config[0]: # conv\n",
    "                conv_match = True\n",
    "                for i in range(len(new_config[1])):\n",
    "                    if new_config[1][i] != config[1][i]: # cDims\n",
    "                        cDims_match = False\n",
    "            dense_match = False\n",
    "            dDims_match = True\n",
    "            if new_config[2] == config[2]: # dense\n",
    "                dense_match = True\n",
    "                for i in range(len(new_config[3])):\n",
    "                    if new_config[3][i] != config[3][i]: # dDims\n",
    "                        dDims_match = False\n",
    "            dropout_match = False\n",
    "            if new_config[4] == config[4]: # dropout\n",
    "                 dropout_match = True\n",
    "            if conv_match and cDims_match and dense_match and dDims_match and dropout_match:\n",
    "                valid = False\n",
    "                break\n",
    "        if valid and isArchitectureValid(input_shape, output_shape, cDims, dDims, dropout):\n",
    "            existing_configs.append(new_config)\n",
    "            return new_config[1], new_config[3], new_config[4]\n",
    "        else:\n",
    "            return mutate_unique(cDims, dDims, existing_configs, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42d58af",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470380f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du jeu de données d'entraînement et de test\n",
    "X_train, y_train, X_test, y_test = load_dataset()\n",
    "\n",
    "# Prétraitement des données : nettoyage et normalisation\n",
    "X_train, X_test = clean_data(X_train, X_test)\n",
    "\n",
    "pop = [Entity(cDims, dDims, dropout.copy(), input_shape, output_shape, \"0_0\")]  # base pour controle\n",
    "conf = [[len(cDims), cDims.copy(), len(dDims), dDims.copy(), dropout.copy()]]\n",
    "for i in range(num_pop - 1):\n",
    "    pop.append(Entity(*mutate_unique(cDims.copy(), dDims.copy(), conf, dropout), input_shape, output_shape, \"0_\"+str(i+1)))\n",
    "\n",
    "for i in range(num_gen):\n",
    "    # ENTRAINEMENT + EVAL\n",
    "\n",
    "    print(f\"Generation {i+1}\")\n",
    "    prev_best = pop[0]\n",
    "    baseline_accuracy = prev_best.accuracy\n",
    "    for entity in pop:\n",
    "        # Un seul entrainement pour le meilleur precedent\n",
    "        if entity is prev_best and baseline_accuracy is not None:\n",
    "            continue\n",
    "        # En GA: pas de courbes, pas d'histories -> beaucoup moins de RAM\n",
    "        evaluate_model(entity, X_train, y_train, folds=folds, epochs=epochs, keep_histories=False)\n",
    "        # Cleanup par individu\n",
    "        entity.drop_model()\n",
    "\n",
    "    # Clean RAM (génération)\n",
    "    tf_cleanup(close_plots=True)\n",
    "\n",
    "    # SELECTION (A AMELIORER)\n",
    "    if baseline_accuracy is None:\n",
    "        baseline_accuracy = pop[0].accuracy\n",
    "    pop = [entity for entity in pop if entity.accuracy > baseline_accuracy]\n",
    "    if (len(pop) == 0): # si rien est strictement meilleur, on garde prev_best \n",
    "        pop = [prev_best]\n",
    "    # on trie par simplicite\n",
    "    pop.sort(key=lambda x: x.complexity)\n",
    "    # on choisi le plus simple\n",
    "    best_entities = [pop[0]]\n",
    "\n",
    "    print(\"Best entity :\")\n",
    "    print(f\"Accuracy : {best_entities[0].accuracy:.3f}, Complexity : {best_entities[0].complexity:.3f}\")\n",
    "    print(best_entities[0].summary())\n",
    "    print(f\"param best: conv={best_entities[0].cLayers}, cDims={best_entities[0].cDims}, dense={best_entities[0].dLayers}, dDims={best_entities[0].dDims}, dropout={best_entities[0].dropout}\")\n",
    "\n",
    "    best_entities[0].drop_model()\n",
    "\n",
    "    # MUTATRON !!!!\n",
    "\n",
    "    # generer new pop en mutant la best\n",
    "    pop = [best_entities[0]]\n",
    "    conf = [[best_entities[0].cLayers, best_entities[0].cDims.copy(), best_entities[0].dLayers, best_entities[0].dDims.copy(), best_entities[0].dropout.copy()]]\n",
    "    for j in range(num_pop - 1):\n",
    "        pop.append(Entity(\n",
    "            *mutate_unique(\n",
    "                best_entities[0].cDims.copy(),\n",
    "                best_entities[0].dDims.copy(),\n",
    "                conf,\n",
    "                best_entities[0].dropout.copy()\n",
    "            ),\n",
    "            input_shape,\n",
    "            output_shape,\n",
    "            str(i+1)+\"_\"+str(j+1)\n",
    "        ))\n",
    "\n",
    "print(\"Final best entity :\")\n",
    "print(f\"Accuracy : {best_entities[0].accuracy:.3f}, Complexity : {best_entities[0].complexity:.3f}\")\n",
    "print(best_entities[0].summary())\n",
    "print(f\"param best: conv={best_entities[0].cLayers}, cDims={best_entities[0].cDims}, dense={best_entities[0].dLayers}, dDims={best_entities[0].dDims}, dropout={best_entities[0].dropout}\")\n",
    "best_entities[0].drop_model()\n",
    "tf_cleanup(close_plots=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
